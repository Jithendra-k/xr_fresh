

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Utilities to work with Dask &mdash; xr_fresh 0.0.0 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/graphviz.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: white" >
          

          
            <a href="index.html" class="icon icon-home" alt="Documentation Home"> xr_fresh
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <!-- Local TOC -->
              <div class="local-toc"><ul>
<li><a class="reference internal" href="#">Utilities to work with Dask</a></li>
<li><a class="reference internal" href="#module-xr_fresh.backends">Functions</a></li>
</ul>
</div>
            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">xr_fresh</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Utilities to work with Dask</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/backends.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="utilities-to-work-with-dask">
<h1>Utilities to work with Dask<a class="headerlink" href="#utilities-to-work-with-dask" title="Permalink to this headline">¶</a></h1>
<div class="toctree-wrapper compound">
</div>
</div>
<div class="section" id="module-xr_fresh.backends">
<span id="functions"></span><h1>Functions<a class="headerlink" href="#module-xr_fresh.backends" title="Permalink to this headline">¶</a></h1>
<dl class="py class">
<dt id="xr_fresh.backends.Cluster">
<em class="property">class </em><code class="sig-prename descclassname">xr_fresh.backends.</code><code class="sig-name descname">Cluster</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/xr_fresh/backends.html#Cluster"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#xr_fresh.backends.Cluster" title="Permalink to this definition">¶</a></dt>
<dd><p>Wrapper for <code class="docutils literal notranslate"><span class="pre">dask</span></code> clients
Best practices:</p>
<blockquote>
<div><dl>
<dt>“By “node” people typically mean a physical or virtual machine. That node can run several programs or processes</dt><dd><p>at once (much like how my computer can run a web browser and text editor at once). Each process can parallelize
within itself with many threads. Processes have isolated memory environments, meaning that sharing data within
a process is free, while sharing data between processes is expensive.
Typically things work best on larger nodes (like 36 cores) if you cut them up into a few processes, each of
which have several threads. You want the number of processes times the number of threads to equal the number
of cores. So for example you might do something like the following for a 36 core machine:</p>
<blockquote>
<div><p>Four processes with nine threads each
Twelve processes with three threads each
One process with thirty-six threads</p>
</div></blockquote>
<p>Typically one decides between these choices based on the workload. The difference here is due to Python’s
Global Interpreter Lock, which limits parallelism for some kinds of data. If you are working mostly with
Numpy, Pandas, Scikit-Learn, or other numerical programming libraries in Python then you don’t need to worry
about the GIL, and you probably want to prefer few processes with many threads each. This helps because it
allows data to move freely between your cores because it all lives in the same process. However, if you’re
doing mostly Pure Python programming, like dealing with text data, dictionaries/lists/sets, and doing most of
your computation in tight Python for loops then you’ll want to prefer having many processes with few threads
each. This incurs extra communication costs, but lets you bypass the GIL.
In short, if you’re using mostly numpy/pandas-style data, try to get at least eight threads or so in a process.
Otherwise, maybe go for only two threads in a process.”
–MRocklin (<a class="reference external" href="https://stackoverflow.com/questions/51099685/best-practices-in-setting-number-of-dask-workers">https://stackoverflow.com/questions/51099685/best-practices-in-setting-number-of-dask-workers</a>)</p>
</dd>
</dl>
</div></blockquote>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># I/O-heavy task with 8 nodes</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cluster</span> <span class="o">=</span> <span class="n">Cluster</span><span class="p">(</span><span class="n">n_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                  <span class="n">threads_per_worker</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                  <span class="n">scheduler_port</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                  <span class="n">processes</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Task with little need of the GIL with 16 nodes</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cluster</span> <span class="o">=</span> <span class="n">Cluster</span><span class="p">(</span><span class="n">n_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                  <span class="n">threads_per_worker</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                  <span class="n">scheduler_port</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                  <span class="n">processes</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<blockquote>
<div><p>When do I use workers versus threads? This probably depends on the problem being executed. If the computation
task is mainly performing many reads at the chunk level (i.e., I/O bound) and the chunk-level process is
relatively simple (i.e., the worker is not spending much time on each chunk) or the process can release the GIL,
more n_threads might be more efficient. If the chunk-level computation is complex (i.e., CPU bound) and is the
main bottleneck, more n_workers might be more efficient. See Dask single-machine for more details about threads
vs. processes.</p>
</div></blockquote>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#xr_fresh.backends.Cluster.start_large_IO_object" title="xr_fresh.backends.Cluster.start_large_IO_object"><code class="xref py py-obj docutils literal notranslate"><span class="pre">start_large_IO_object</span></code></a>(self)</p></td>
<td><p>Using few processes and many threads per process is good if you are doing mostly numeric workloads, such as are common in Numpy, Pandas, and Scikit-Learn code, which is not affected by Python’s Global Interpreter Lock (GIL).</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#xr_fresh.backends.Cluster.start_large_object" title="xr_fresh.backends.Cluster.start_large_object"><code class="xref py py-obj docutils literal notranslate"><span class="pre">start_large_object</span></code></a>(self)</p></td>
<td><p>Using few processes and many threads per process is good if you are doing mostly numeric workloads, such as are common in Numpy, Pandas, and Scikit-Learn code, which is not affected by Python’s Global Interpreter Lock (GIL).</p></td>
</tr>
</tbody>
</table>
<table class="docutils align-default">
<colgroup>
<col style="width: 69%" />
<col style="width: 31%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>close</strong></p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><strong>restart</strong></p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><strong>start</strong></p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><strong>start_small_object</strong></p></td>
<td></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="xr_fresh.backends.Cluster.start_large_IO_object">
<code class="sig-name descname">start_large_IO_object</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/xr_fresh/backends.html#Cluster.start_large_IO_object"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#xr_fresh.backends.Cluster.start_large_IO_object" title="Permalink to this definition">¶</a></dt>
<dd><p>Using few processes and many threads per process is good if you are doing mostly
numeric workloads, such as are common in Numpy, Pandas, and Scikit-Learn code,
which is not affected by Python’s Global Interpreter Lock (GIL).
Rasterio also releases GIL <a class="reference external" href="https://rasterio.readthedocs.io/en/latest/topics/concurrency.html">https://rasterio.readthedocs.io/en/latest/topics/concurrency.html</a></p>
</dd></dl>

<dl class="py method">
<dt id="xr_fresh.backends.Cluster.start_large_object">
<code class="sig-name descname">start_large_object</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/xr_fresh/backends.html#Cluster.start_large_object"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#xr_fresh.backends.Cluster.start_large_object" title="Permalink to this definition">¶</a></dt>
<dd><p>Using few processes and many threads per process is good if you are doing mostly
numeric workloads, such as are common in Numpy, Pandas, and Scikit-Learn code,
which is not affected by Python’s Global Interpreter Lock (GIL).
Rasterio also releases GIL <a class="reference external" href="https://rasterio.readthedocs.io/en/latest/topics/concurrency.html">https://rasterio.readthedocs.io/en/latest/topics/concurrency.html</a></p>
</dd></dl>

</dd></dl>

</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, Michael Mann

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-XXXXXXX-1', 'auto');
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>